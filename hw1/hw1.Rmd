---
title: "hw1"
author: "Xingwen Wei"
date: "February 3, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Question 1**
```{r}
library(ggplot2)

x <- c(1:20)
bias <- 1/x +0.1
variance <- x^1.5/100 + 0.1
Bayes_error <- 0.2
test_error <- bias + variance + Bayes_error
training_error <- x^(-0.5) - 0.2

ggplot(data=data.frame(x, bias, variance, Bayes_error, training_error, test_error)) +xlab("Model Complexity") + ylab("Value") + geom_smooth(aes(x=x, y=bias, color = 'bias'), method=loess, se=F) + geom_smooth(aes(x=x, y=variance, color = 'variance'), method=loess, se=F) + geom_smooth(aes(x=x, y=Bayes_error, color = 'Bayes error'), method=loess, se=F) + geom_smooth(aes(x=x, y=test_error,  color = 'test error'), method=loess, se=F) +  geom_smooth(aes(x=x, y=training_error,color = 'training error'), method=loess, se=F) + scale_color_manual(name="Legend", values = c("Bayes error"="blue", "training error"='red', "test error"='orange', "bias"='black', "variance"='purple')) + scale_x_continuous(breaks=c(5, 15), labels=c("less flexible", "more flexible")) + theme(axis.ticks = element_blank(), axis.text.y = element_blank())

```

**Question 2**

**a**. 
When we have a large sized sample and a few predictors, we would expect the performance of a flexible method to be better. 
The model variance will be small for all model because of the large sample size.
The bias will be smaller with a more flexible model.
Since we cannot do anything about the Bayes error, we should choose a more flexible model in this situation to achieve a better test error. 

**b**. 
When we have a small sized sample and many predictors, we would expect the performance of an inflexible method to be better. 
The model variance will be large for all model because of the small sample size.
The bias will be smaller with a more flexible model.
However, since the sample size is small, there is high chance that the sample is not representative and the model learned from this sample will be far from the true model even with high flexibility. 
Thus, we should choose an inflexible model in this situation to achieve a better test error. 


**c**. 
When the predictor and response have a highly non-linear relationship, we would expect the performance of a flexible method to be better. 
We assume we have a large sample size because it will take a large sample to reach the conclusion that the relationship between predictor and response is not linear.
The model variance will be small for all model because of the large sample size.
The bias will be smaller with a more flexible model.
Moreover, the non-linear relationship will be better represented in a more flexible model.
Thus, we should choose a flexible model in this situation to achieve a better test error. 
**d**. 
When the variance of the error term is large, we have to get a model not too flexible and not too inflexible.
We cannot decide whether a flexible or inflexible model will be better with only the given information. 

**Question 3**

**a**. 
```{r}
dist <- function(t, x){
  sqrt((t[1]-x[1])^2+(t[2]-x[2])^2+(t[3]-x[3])^2)
}

pred <- function(K){
  t <- table(obs[1:K,4])
  cat(sprintf("Prediction: %s", names(t[which.max(t)])))
}

test <- c(0,0,0)
c1 <- list(0,3,0, 'red')
c2 <- list(2,0,0, 'red')
c3 <- list(0,1,3, 'red')
c4 <- list(0,1,2, 'green')
c5 <- list(-1,0,1, 'green')
c6 <- list(1,1,2, 'red')
obs <- as.data.frame(rbind(c1, c2, c3, c4, c5, c6))
obs[4] <- as.factor(unlist(obs[4]))

dst <- c()

for(x in (1:6)){
  d <- dist(test, unname(unlist(obs[x,1:3])))
  dst[x] <- d
  cat(sprintf("Euclidean distance to observation %d is: %f \n", x, d))}

obs <- cbind(obs, dst)
obs <- obs[order(obs[,5]),]

K <- 3

pred(K)


```

**b**. 
When we have large training set and nonlinear relationship between predictor and response, we expect the best value for K to be small.
As mentioned above on Question 2c, we want a more flexible model to represent the nonlinear relationship while maintaining a low model variance because of the large training sample size. 


**Question 4**

**Data Preprocessing**. 
```{r}
train <- read.csv("C:/Users/xingw/Desktop/503/stats503/hw1/diabetes_train.csv", header=T)
test <- read.csv("C:/Users/xingw/Desktop/503/stats503/hw1/diabetes_test.csv", header=T)

```

To get familiar with the data, we want to see a data summary first.
Noticing the outcome is a binary label, we set it as a factor.
Also, after detecting apparent anomalies in attribute "Glucose", "BloodPressure", "SkinThickness", "Insulin", and "BMI", we replace the impossible 0's with NA. 
We have noted that about half of the observations for attribute "Insulin" and "SkinThickness" are missing, so we decided to not include this predictor in our model.
Then we remove all the observations that is not complete. 

```{r}
train$Outcome <- factor(train$Outcome)


summary(train)

train$Glucose[train$Glucose == 0] <- NA
train$BloodPressure[train$BloodPressure == 0] <- NA
train$SkinThickness[train$SkinThickness == 0] <- NA
train$Insulin[train$Insulin == 0] <- NA
train$BMI[train$BMI == 0] <- NA


summary(train)

train_c <- train[complete.cases(train[,-c(4, 5)]),-c(4, 5)]


```

**Exploratory Data Analysis**. 
From the correlation matrix of variables, we do not find any obvious signs for collinearities.

```{r}
library(GGally)

```

In order to understand the relationship between each predictor variable and the response, we use boxplot for each predictor against the outcome. 
According to the boxplot, predictor variables "Pregnancies" and "Glucose" are more useful in predicting the outcome than other predictors. 
```{r}
library(tidyr)

explore <- gather(data=train_c, key="Predictor", value="Value", -"Outcome")
ggplot(data=explore, aes(x=Outcome, y=Value)) + geom_boxplot() + facet_wrap(.~Predictor, scales='free_y') 


```

**KNN**. 

In order to get most information, we use PMM to impute missing data in the test file after removing the two predictor variables not used in the training set.
According to the KNN error plot, we would prefer $K=6$ as it gives the minimum testing error. 


```{r}
library(mice)
library(class)

# Standardize train data
train_label <- train_c$Outcome
train_x <- train_c[-7]
train_mu <- colMeans(train_x)
train_std <- sqrt(diag(var(train_x)))
train_x <- scale(train_x, center=train_mu, scale=train_std)

# Preprocess test data
test$Glucose[test$Glucose == 0] <- NA
test$BloodPressure[test$BloodPressure == 0] <- NA
test$SkinThickness[test$SkinThickness == 0] <- NA
test$Insulin[test$Insulin == 0] <- NA
test$BMI[test$BMI == 0] <- NA
# Impute missing test data
test_temp <- mice(data=test[,-c(4, 5)], m=1, method='pmm')
test_c <- complete(test_temp)
# Standardize test data
test$Outcome <- factor(test$Outcome)
test_label <- test$Outcome
test_x <- test_c[-7]
test_x <- scale(test_x, center=train_mu, scale=train_std)

# KNN prediction
k_range <- 1:20
train_error <- c()
test_error <- c()

for(this_k in k_range){
  pred_train <- knn(train=train_x, test=train_x, cl=train_label, k=this_k)
  pred_test <- knn(train=train_x, test=test_x, cl=train_label, k=this_k)
  train_error[this_k] <- mean(pred_train != train_label)
  test_error[this_k] <- mean(pred_test != test_label)
  # Evaluation
  # table(pred, test_label)
}
  
result <- data.frame(train_error, test_error, k_range)

ggplot(result, aes(x=1/k_range)) + geom_line(aes(y=train_error, color="train")) + geom_point(aes(y=train_error, color="train")) + geom_line(aes(y=test_error, color="test")) + geom_point(aes(y=test_error, color="test")) + xlab("Model Complexity (1/K)") + ylab("Error Rate") + scale_color_manual(name="Legend", values = c("test"="blue", "train"="red"))
```






































